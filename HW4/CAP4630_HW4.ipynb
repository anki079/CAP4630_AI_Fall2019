{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAP4630_HW4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anki079/CAP4630_AI_Fall2019/blob/master/HW4/CAP4630_HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9USX6M5LiKT",
        "colab_type": "text"
      },
      "source": [
        "#CAP 4630: Homework 4, Fall 2019\n",
        "\n",
        "####Name: Ankita Tripathi\n",
        "\n",
        "The goal of this assignment is to provide a summary and description of the different concepts, methods, and algorithms that have been learned over the duration of this course. \n",
        "\n",
        "**Note**: Some of the code snippets included in this notebook have been taken from course materials provided by Dr Wocjan. These course materials can be found at: https://github.com/schneider128k/machine_learning_course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8hWAN0q53s0",
        "colab_type": "text"
      },
      "source": [
        "#**0. General Concepts**\n",
        "\n",
        "The distinction between the fields of Artificial Intelligence, Machine Learning, and Deep Learning was made clear in this class. \n",
        "It was understood that Machine Learning fell under the umbrella of Artificial Intelligence. Deep Learning was further encapsulated in Machine Learning. \n",
        "\n",
        "Note: Some of the following concepts described in this section were beyond the scope of this course and thus were not covered in detail.\n",
        "\n",
        "###**Artificial Intelligence (AI)**\n",
        "Artificial Intelligence may be defined as a computer system that is able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages. It deals with the simulation of intelligent behavior in computers. The collection of all methods in AI research that are based on symbolic representations of problems, logic and, search is called symbolic AI or GOFAI (\"Good Old-Fashioned AI\"). It can be thought of as a system that takes in an input and a set of rules and produces an output.\n",
        "\n",
        "Input ----------> +-------------+           \n",
        "\n",
        ">>> |...............| ----------> Output\n",
        "\n",
        "Rules ---------->  +------------+\n",
        "\n",
        "###**Machine Learning (ML)**\n",
        "Machine Learning allows computers to make predictions without any explicit programming. ML algorithms are set up to adjust their output based on the data they are passed. Machine Learning does not require human interference to make changes. Unlike symbolic AI, machine learning has the ability to modify itself when more data is presented to it.\n",
        "\n",
        "Input ----------> +-----------+           \n",
        "                   \n",
        ">>>  |.............| ----------> Rules\n",
        "\n",
        "Output ------->  +------------+\n",
        "\n",
        "\n",
        "###**Deep Learning (DL)**\n",
        "Deep Learning is a subset of ML wherein the algorithms roughly try to mimic the processing patterns that occur in the human brain. For instance, ML requires the features to be given to the program while a deep learning model can discover the features required for classification by itself.\n",
        "\n",
        "###**Supervised Learning**\n",
        "Supervised learning is a branch of ML where the model is provided with labeled training data. It finds patterns between the data and labels and expresses them as mathematical functions. For a given input feature, the system is explicitly told what the expected output label is, hence the name Supervised Learning.\n",
        "\n",
        "###**Unsupervised Learning**\n",
        "The goal of unsupervised learning is to identify meaningful patterns in the data. To that end, the program needs to learn from an unlabeled dataset. The model is not given any rules and must self-infer them to categorize the data given to it.\n",
        "\n",
        "###**Reinforcement Learning (RL)**\n",
        "In RL, the model (also known as \"agent\") proceeds in an environment where it can be in a certain number of states, and each state has an action and reward associated with it, on the basis of which the agent can transition into another state. Generally the goal of RL is for the agent to learn which actions to take in order to maximize the total reward it receives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh0SM6rK54Iq",
        "colab_type": "text"
      },
      "source": [
        "#**1. Building a model** \n",
        "\n",
        "The structure of a neural network is loosely analogous to the networks of neurons found in human brains where one group of neurons firing triggers activity in some other neurons. A neural network in AI is composed of a series of layers. The most basic neural network will have an input layer, an intermediate hidden layer, and an output layer. The hidden layer can be of different types depending on the purpose and architecture of the neural network. \n",
        "\n",
        "![](https://i.imgur.com/1xyl55E.jpg)\n",
        "\n",
        "In a **convolutional neural network** (CNN), the hidden layers can be thought of as a set of features based on the previous layer. Successive hidden layers are higher features based on previous features. \n",
        "\n",
        "![](https://miro.medium.com/max/2510/1*vkQ0hXDaQv57sALXAJquxA.jpeg)\n",
        "\n",
        "The input to the CNN is a feature map as a 3-D matrix, where the first 2 dimensions are the length and width of the images in pixels and the third dimension is the number of channels in an image.\n",
        "\n",
        "The CNN comprises a stack of modules, wherein each stack performs 3 operations, as follows:\n",
        "1. _Convolution_: A convolution extracts tiles of the input feature map, and\n",
        "applies filters to them to compute new features, producing an output feature map, or convolved feature (which may have a different size and depth than the input feature map). This operation is performed by essentially sliding a window known as a \"kernel\" over the input image, and computing a dot product of the vectors to extract features.\n",
        "2. _ReLU_: After each convolution operation, the CNN applies a Rectified\n",
        "Linear Unit (ReLU) transformation to the convolved feature, in order to introduce nonlinearity into the model.\n",
        "3. _Pooling_: After ReLU comes a pooling step, in which the CNN downsamples the convolved feature (to save on processing time), reducing the number of dimensions of the feature map, while still preserving the most critical feature information. A common algorithm to perform this is called max pooling.\n",
        "\n",
        "At the end of a CNN there is typically one (or more) dense layer (a layer in which every node in that layer is connected to every node in the next layer). These layers perform classification based on the features extracted by the convolutions. \n",
        "\n",
        "Typically, the final fully connected layer contains a softmax activation function, which outputs a probability value from 0 to 1 for each of the classification labels the model is trying to predict.\n",
        "\n",
        "The following code snippet is an example implementation of how to build a CNN in keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1ggKa17T2s5",
        "colab_type": "code",
        "outputId": "e6670b3d-63c1-4b46-d413-11dba628815e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NHOTKzj54aV",
        "colab_type": "text"
      },
      "source": [
        "#**2. Compiling a model** \n",
        "\n",
        "In Keras, we can compile a model as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHp0sz7cYPsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network.compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNW9llmdkiuQ",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the compile() function takes a number of arguments. The required arguments from this list are _optimizer, loss_, and _metrics_. \n",
        "\n",
        "* **optimizer** : This is a string (name of optimizer) or optimizer instance.\n",
        "\n",
        ">> Most deep learning algorithms involve optimization of some sort. Optimization refers to the task of either minimizing or maximizing some function f(x) by altering x. Examples of optimizers are the Stochastic Gradient Descent (SGD) and RMSprop functions. Optimizers depend on various parameters but one parameter common to all optimizers is the learning rate.\n",
        "\n",
        ">> The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. There’s a Goldilocks learning rate for every regression problem.\n",
        "If you know the gradient of the loss function is small then you\n",
        "can safely try a larger learning rate, which compensates for\n",
        "the small gradient and results in a larger step size.\n",
        "\n",
        "* **loss** : This is a string (name of objective function) or objective function or Loss instance. \n",
        "\n",
        ">> Loss is the penalty for a bad prediction, i.e it is a number indicating how bad the model’s prediction was on a single example. If the model’s prediction is perfect, the loss is zero; otherwise, the loss is greater.\n",
        "The goal of training a model is to find a set of weights and\n",
        "biases that have low loss, on average, across all examples. Examples of  loss functions are mean squared error (MSE), categorical crossentropy, hinge etc.\n",
        "\n",
        "* **metrics** : This is a list of metrics to be evaluated by the model during training and testing. \n",
        "\n",
        ">> Typically you will use metrics=['accuracy']. To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}. You can also pass a list (len = len(outputs)) of lists of metrics such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']].\n",
        "\n",
        "The following code snippet is an example implementation of the compilation step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTVWbLpElO4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRK3gDHc55Af",
        "colab_type": "text"
      },
      "source": [
        "#**3. Training a model**\n",
        "\n",
        "Training a model simply means learning (determining) good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called empirical risk minimization. \n",
        "\n",
        "At this step the model is essentially looking at the input data, making a prediciton, and checking if it is right or wrong. The training step takes place after we have built and compiled our model.\n",
        "\n",
        "The following code snippet is an example implementation of the training step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YuVq3JIZksS",
        "colab_type": "code",
        "outputId": "9a1b0552-1300-4736-c4b3-218684fdbcee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "epochs = 10\n",
        "history = model.fit(train_images, \n",
        "                    train_labels, \n",
        "                    epochs=epochs, \n",
        "                    batch_size=64,\n",
        "                    validation_data=(test_images, test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 0.1613 - acc: 0.9502 - val_loss: 0.0466 - val_acc: 0.9852\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 10s 166us/step - loss: 0.0458 - acc: 0.9860 - val_loss: 0.0403 - val_acc: 0.9875\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 10s 166us/step - loss: 0.0316 - acc: 0.9906 - val_loss: 0.0263 - val_acc: 0.9917\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 10s 167us/step - loss: 0.0243 - acc: 0.9928 - val_loss: 0.0255 - val_acc: 0.9920\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0187 - acc: 0.9947 - val_loss: 0.0305 - val_acc: 0.9915\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0158 - acc: 0.9953 - val_loss: 0.0285 - val_acc: 0.9917\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0125 - acc: 0.9963 - val_loss: 0.0298 - val_acc: 0.9925\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.0100 - acc: 0.9969 - val_loss: 0.0430 - val_acc: 0.9910\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0090 - acc: 0.9973 - val_loss: 0.0464 - val_acc: 0.9914\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 10s 166us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0404 - val_acc: 0.9924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAaTa_7e1ick",
        "colab_type": "text"
      },
      "source": [
        "Generally once training has concluded, it may be observed that the training loss decreased with every epoch, and the training accuracy increased with every epoch. That is to be expected with gradient descent optimization - the quantity you're trying to minimize should be less with every iteration.\n",
        "\n",
        "* **Underfitting** : An observation may be made wherein the model fails to \"learn\" from the data and cannot make reliable predictions. The model may be too simple, resulting in low generalization. This is called underfitting.\n",
        "\n",
        "* **Overfitting** : In contrast, another observation may be made wherein the training accuracy increases with every epoch but the test accuracy may stagnate after increasing up to a certain point. This means that the model failed to generalize to data outside the training set an, and instead learned representations specific to the training data. The model overoptimized on the training set. This is called overfitting. We can use techniques such as data augmentation and dropout to combat overfitting. \n",
        "\n",
        ">>Data augmentation is a strategy that enables significantly increasing the diversity of data available for training models, without actually collecting new data. \n",
        "\n",
        ">>Dropout is a process by which a single model can be used to simulate having a large number of different network architectures by randomly dropping out nodes during training. \n",
        "\n",
        "![](https://i.imgur.com/6CigYyb.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHyMZK-C55ue",
        "colab_type": "text"
      },
      "source": [
        "# **4. Finetuning a pretrained model**\n",
        "\n",
        "After a model has been trained, it may be possible to increase its performance even further. One way to do this is to \"fine-tune\" the weights of the top layers of the pretrained model alongside the training of the top-level classifier. \n",
        "\n",
        "It must be noted that fine-tuning has the following caveats:\n",
        "* Fine-tuning should be attempted only after the top-level classifier has been trained with the pretrained model set to non-trainable.\n",
        "* Additionally, only the top layers of the pre-trained model are fine-tuned rather than all layers of the pretrained model because in a convnet, a layer is more specialized the higher up it is.\n",
        "\n",
        "As we go higher up, the features are increasingly specific to the dataset that the model is trained on. In order to fine-tune a model, we need to adapt these specialized features to work with the new dataset. \n",
        "\n",
        "We proceed to fine-tune a model by setting the top layers of the pretrained model to be trainable, then recompiling the model so that the changes to the top layers take effect, and resuming the training.\n",
        "In addition to this, using a smaller learning rate is also beneficial, since we expect the pre-trained weights to be quite good already as compared to randomly initialized weights, and we do not want to distort them too quickly and too much.\n",
        "\n",
        "The following code snippet is an example implementation of how to fine-tune a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwkVMx8N8qMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block5_conv1':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyN-J7rs9cvo",
        "colab_type": "code",
        "outputId": "c8b2249f-45f4-4236-f0d4-50d92586d50b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3377
        }
      },
      "source": [
        "# compile model\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    #\n",
        "    # choose a smaller learning rate\n",
        "    #\n",
        "    optimizer=optimizers.RMSprop(lr=1e-5), \n",
        "    metrics=['acc'])\n",
        "\n",
        "# train\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - 29s 290ms/step - loss: 0.2807 - acc: 0.8780 - val_loss: 0.2615 - val_acc: 0.9050\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.2686 - acc: 0.8885 - val_loss: 0.2210 - val_acc: 0.9110\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.2290 - acc: 0.9020 - val_loss: 0.2199 - val_acc: 0.9180\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.2329 - acc: 0.9075 - val_loss: 0.2041 - val_acc: 0.9240\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.2207 - acc: 0.9040 - val_loss: 0.2134 - val_acc: 0.9230\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.2019 - acc: 0.9210 - val_loss: 0.1870 - val_acc: 0.9240\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1927 - acc: 0.9195 - val_loss: 0.1558 - val_acc: 0.9450\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1731 - acc: 0.9275 - val_loss: 0.3439 - val_acc: 0.8950\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.1661 - acc: 0.9375 - val_loss: 0.1739 - val_acc: 0.9410\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.1529 - acc: 0.9370 - val_loss: 0.2218 - val_acc: 0.9280\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.1508 - acc: 0.9345 - val_loss: 0.2253 - val_acc: 0.9280\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.1392 - acc: 0.9470 - val_loss: 0.1825 - val_acc: 0.9400\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1370 - acc: 0.9480 - val_loss: 0.2039 - val_acc: 0.9330\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.1398 - acc: 0.9435 - val_loss: 0.1915 - val_acc: 0.9380\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 27s 270ms/step - loss: 0.1138 - acc: 0.9515 - val_loss: 0.1760 - val_acc: 0.9410\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.1199 - acc: 0.9505 - val_loss: 0.3001 - val_acc: 0.9110\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.1121 - acc: 0.9575 - val_loss: 0.1918 - val_acc: 0.9410\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.1086 - acc: 0.9595 - val_loss: 0.2275 - val_acc: 0.9370\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.1089 - acc: 0.9585 - val_loss: 0.2696 - val_acc: 0.9180\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0929 - acc: 0.9615 - val_loss: 0.1932 - val_acc: 0.9410\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - 27s 270ms/step - loss: 0.0900 - acc: 0.9635 - val_loss: 0.1862 - val_acc: 0.9370\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - 27s 270ms/step - loss: 0.0856 - acc: 0.9645 - val_loss: 0.2677 - val_acc: 0.9260\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0760 - acc: 0.9710 - val_loss: 0.3044 - val_acc: 0.9250\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0764 - acc: 0.9725 - val_loss: 0.1971 - val_acc: 0.9470\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0713 - acc: 0.9770 - val_loss: 0.2527 - val_acc: 0.9340\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0775 - acc: 0.9680 - val_loss: 0.1997 - val_acc: 0.9380\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0715 - acc: 0.9725 - val_loss: 0.1841 - val_acc: 0.9450\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - 27s 270ms/step - loss: 0.0796 - acc: 0.9670 - val_loss: 0.2349 - val_acc: 0.9390\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - 27s 270ms/step - loss: 0.0761 - acc: 0.9675 - val_loss: 0.2163 - val_acc: 0.9380\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0696 - acc: 0.9735 - val_loss: 0.1941 - val_acc: 0.9430\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0630 - acc: 0.9770 - val_loss: 0.2935 - val_acc: 0.9210\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0757 - acc: 0.9725 - val_loss: 0.2635 - val_acc: 0.9300\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0645 - acc: 0.9745 - val_loss: 0.2183 - val_acc: 0.9430\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0427 - acc: 0.9825 - val_loss: 0.1534 - val_acc: 0.9520\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0550 - acc: 0.9800 - val_loss: 0.3282 - val_acc: 0.9300\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0636 - acc: 0.9790 - val_loss: 0.2576 - val_acc: 0.9410\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0642 - acc: 0.9760 - val_loss: 0.3076 - val_acc: 0.9250\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0502 - acc: 0.9820 - val_loss: 0.3608 - val_acc: 0.9290\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.0483 - acc: 0.9805 - val_loss: 0.3187 - val_acc: 0.9360\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0540 - acc: 0.9800 - val_loss: 0.2250 - val_acc: 0.9390\n",
            "Epoch 41/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0504 - acc: 0.9815 - val_loss: 0.3995 - val_acc: 0.9190\n",
            "Epoch 42/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0404 - acc: 0.9885 - val_loss: 0.1875 - val_acc: 0.9530\n",
            "Epoch 43/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.0530 - acc: 0.9810 - val_loss: 0.2738 - val_acc: 0.9350\n",
            "Epoch 44/100\n",
            "100/100 [==============================] - 26s 264ms/step - loss: 0.0403 - acc: 0.9850 - val_loss: 0.2369 - val_acc: 0.9440\n",
            "Epoch 45/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0430 - acc: 0.9830 - val_loss: 0.2627 - val_acc: 0.9370\n",
            "Epoch 46/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0431 - acc: 0.9840 - val_loss: 0.2256 - val_acc: 0.9420\n",
            "Epoch 47/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0526 - acc: 0.9795 - val_loss: 0.3218 - val_acc: 0.9350\n",
            "Epoch 48/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0388 - acc: 0.9875 - val_loss: 0.2341 - val_acc: 0.9430\n",
            "Epoch 49/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0396 - acc: 0.9845 - val_loss: 0.2010 - val_acc: 0.9480\n",
            "Epoch 50/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0355 - acc: 0.9885 - val_loss: 0.2544 - val_acc: 0.9430\n",
            "Epoch 51/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0335 - acc: 0.9860 - val_loss: 0.2016 - val_acc: 0.9540\n",
            "Epoch 52/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0371 - acc: 0.9850 - val_loss: 0.2506 - val_acc: 0.9460\n",
            "Epoch 53/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0305 - acc: 0.9875 - val_loss: 0.2550 - val_acc: 0.9430\n",
            "Epoch 54/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0326 - acc: 0.9845 - val_loss: 0.2175 - val_acc: 0.9610\n",
            "Epoch 55/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0218 - acc: 0.9925 - val_loss: 0.2022 - val_acc: 0.9570\n",
            "Epoch 56/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0396 - acc: 0.9860 - val_loss: 0.2207 - val_acc: 0.9520\n",
            "Epoch 57/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0450 - acc: 0.9840 - val_loss: 0.3072 - val_acc: 0.9400\n",
            "Epoch 58/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0293 - acc: 0.9890 - val_loss: 0.2114 - val_acc: 0.9520\n",
            "Epoch 59/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0389 - acc: 0.9865 - val_loss: 0.1939 - val_acc: 0.9530\n",
            "Epoch 60/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0386 - acc: 0.9865 - val_loss: 0.3334 - val_acc: 0.9300\n",
            "Epoch 61/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0249 - acc: 0.9920 - val_loss: 0.2643 - val_acc: 0.9530\n",
            "Epoch 62/100\n",
            "100/100 [==============================] - 27s 270ms/step - loss: 0.0322 - acc: 0.9855 - val_loss: 0.2531 - val_acc: 0.9500\n",
            "Epoch 63/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0369 - acc: 0.9870 - val_loss: 0.2709 - val_acc: 0.9420\n",
            "Epoch 64/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0229 - acc: 0.9915 - val_loss: 0.2287 - val_acc: 0.9600\n",
            "Epoch 65/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0260 - acc: 0.9885 - val_loss: 0.1986 - val_acc: 0.9590\n",
            "Epoch 66/100\n",
            "100/100 [==============================] - 27s 270ms/step - loss: 0.0327 - acc: 0.9865 - val_loss: 0.2493 - val_acc: 0.9510\n",
            "Epoch 67/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0308 - acc: 0.9890 - val_loss: 0.3151 - val_acc: 0.9400\n",
            "Epoch 68/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0267 - acc: 0.9905 - val_loss: 0.2130 - val_acc: 0.9500\n",
            "Epoch 69/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0265 - acc: 0.9880 - val_loss: 0.2440 - val_acc: 0.9540\n",
            "Epoch 70/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0235 - acc: 0.9900 - val_loss: 0.2403 - val_acc: 0.9530\n",
            "Epoch 71/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0311 - acc: 0.9885 - val_loss: 0.3174 - val_acc: 0.9300\n",
            "Epoch 72/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0226 - acc: 0.9930 - val_loss: 0.2940 - val_acc: 0.9350\n",
            "Epoch 73/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0234 - acc: 0.9910 - val_loss: 0.2380 - val_acc: 0.9500\n",
            "Epoch 74/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.0232 - acc: 0.9925 - val_loss: 0.3169 - val_acc: 0.9370\n",
            "Epoch 75/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0259 - acc: 0.9930 - val_loss: 0.3292 - val_acc: 0.9400\n",
            "Epoch 76/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0177 - acc: 0.9940 - val_loss: 0.2653 - val_acc: 0.9460\n",
            "Epoch 77/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.0230 - acc: 0.9905 - val_loss: 0.3764 - val_acc: 0.9350\n",
            "Epoch 78/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0279 - acc: 0.9885 - val_loss: 0.3214 - val_acc: 0.9430\n",
            "Epoch 79/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.0247 - acc: 0.9910 - val_loss: 0.2442 - val_acc: 0.9490\n",
            "Epoch 80/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.0292 - acc: 0.9875 - val_loss: 0.3284 - val_acc: 0.9430\n",
            "Epoch 81/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.0237 - acc: 0.9935 - val_loss: 0.2735 - val_acc: 0.9490\n",
            "Epoch 82/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0264 - acc: 0.9905 - val_loss: 0.2957 - val_acc: 0.9380\n",
            "Epoch 83/100\n",
            "100/100 [==============================] - 26s 264ms/step - loss: 0.0237 - acc: 0.9920 - val_loss: 0.2135 - val_acc: 0.9520\n",
            "Epoch 84/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.0295 - acc: 0.9905 - val_loss: 0.2302 - val_acc: 0.9580\n",
            "Epoch 85/100\n",
            "100/100 [==============================] - 27s 265ms/step - loss: 0.0267 - acc: 0.9930 - val_loss: 0.2763 - val_acc: 0.9520\n",
            "Epoch 86/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.0192 - acc: 0.9930 - val_loss: 0.4515 - val_acc: 0.9320\n",
            "Epoch 87/100\n",
            "100/100 [==============================] - 27s 265ms/step - loss: 0.0227 - acc: 0.9920 - val_loss: 0.2442 - val_acc: 0.9450\n",
            "Epoch 88/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0190 - acc: 0.9940 - val_loss: 0.3292 - val_acc: 0.9390\n",
            "Epoch 89/100\n",
            "100/100 [==============================] - 27s 265ms/step - loss: 0.0172 - acc: 0.9925 - val_loss: 0.2748 - val_acc: 0.9510\n",
            "Epoch 90/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0238 - acc: 0.9885 - val_loss: 0.2622 - val_acc: 0.9530\n",
            "Epoch 91/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0245 - acc: 0.9915 - val_loss: 0.2883 - val_acc: 0.9490\n",
            "Epoch 92/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.0166 - acc: 0.9950 - val_loss: 0.2646 - val_acc: 0.9570\n",
            "Epoch 93/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0212 - acc: 0.9935 - val_loss: 0.2609 - val_acc: 0.9490\n",
            "Epoch 94/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0231 - acc: 0.9915 - val_loss: 0.3442 - val_acc: 0.9460\n",
            "Epoch 95/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.0290 - acc: 0.9905 - val_loss: 0.2704 - val_acc: 0.9480\n",
            "Epoch 96/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0240 - acc: 0.9930 - val_loss: 0.2700 - val_acc: 0.9460\n",
            "Epoch 97/100\n",
            "100/100 [==============================] - 27s 265ms/step - loss: 0.0161 - acc: 0.9950 - val_loss: 0.3633 - val_acc: 0.9430\n",
            "Epoch 98/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0196 - acc: 0.9920 - val_loss: 0.3085 - val_acc: 0.9420\n",
            "Epoch 99/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.0234 - acc: 0.9920 - val_loss: 0.2316 - val_acc: 0.9560\n",
            "Epoch 100/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0153 - acc: 0.9935 - val_loss: 0.3332 - val_acc: 0.9450\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}